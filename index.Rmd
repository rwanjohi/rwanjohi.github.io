---
title: 'Richard''s Website '
---


Welcome to my world...


$$ P(Data|Evidence) =  \frac{P(Evidence|Data) \times P(Data)}{P(Evidence)} $$



My favourite tools:

-  [R](https://www.r-project.org/)

-  Python [Download Anaconda](https://www.anaconda.com/download/)

-  Spark. 

-  [KNIME](https://www.knime.com/)

-  Databricks

-  Tableau: [view my vizs](https://public.tableau.com/profile/richard.wanjohi#!/)


```{r}
summary(cars$dist)
summary(cars$speed)
```

My python snippet

```

import numpy as np
import pandas as pd
from sklearn.cross_validation import train_test_split, StratifiedKFold 
from sklearn.ensemble  import GradientBoostingClassifier as GBC, RandomForestClassifier

from sklearn.model_selection import GridSearchCV

```
\textcolor{blue}{import} 

\begin{abstract}
%In data science world, labelled data is rare and getting it, in most cases, is very impractical or very expensive. 
With careful and  proper training, recent Neural networks models can be able to create new data similar to what we supply them. Generative Adversarial Networks (GAN) is one such model.\\
 GAN consist of two competing  models striving to outdo each other: the Generator and Discriminator models. The Generator takes in random input and tries to generate real data (curves, images, texts, \ldots). This model has no idea what the real data look like. The Discriminator, on the other hand, is a binary classifier that takes as its input the fake data, generated by the Generator, and the real dataset and learn to tell whether the data is real or fake. \\ Through back propagation, the output from the Discriminator is used by the Generator to update its parameters accordingly in order to produce data that can actually fool the Discriminator. The two models are trained simultaneously with  the aim that the continuous  competition will help produce data that is indistinguishable from the real data. \\ 
 We will very briefly introduce the Math behind the GAN and demonstrate its implementation and training. The implementation will involve using both Multilayer perceptron (MLP) and Convolutional Neural Network (CNN) configurations.  The output and code, in Python and R and using Keras with Tensoflow or Theano as backend, will be displayed. 
 
\end{abstract}

